# -*- coding: utf-8 -*-
"""Diabetes_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Tamasha-Ab/Diabetes-Prediction/blob/main/Diabetes_Prediction.ipynb

# Project Title: Diabetes Prediction

*   Group Number: 11
*   Group Members:

*   Perera M.A.J.C. - EG/2021/4711
*   Tamasha A.P.D.  - EG/2021/4823

# **Introduction**

The Diabetes Prediction project uses machine learning algorithms like logistic regression, decision trees, and random forests to assess diabetes risk. It involves preprocessing medical data, selecting important features, and training models to make predictions. By evaluating the performance of these models, the project helps identify individuals who may be at risk of diabetes, offering valuable insights for healthcare predictions and showcasing how machine learning can be applied to medical diagnostics.

# **Literature Survey**

*   Machine Learning in Healthcare: Algorithms like logistic regression, decision trees, and random forests are widely used for medical predictions, offering accurate risk assessment.
*   Data Preprocessing: Techniques like scaling and feature selection improve prediction accuracy by focusing on significant medical attributes (e.g., glucose levels, BMI).
*   Decision Trees: Known for interpretability, decision trees help visualize the reasoning behind predictions, aiding medical professionals.
*   Random Forests: Effective for imbalanced datasets, reducing overfitting, and providing robust predictions.
*   Logistic regression: Frequently used in healthcare for predicting conditions like diabetes based on clinical and demographic data
*   Model Evaluation: Metrics like ROC-AUC and precision-recall are essential for reliable healthcare insights.

# **Dataset Description**

*   Dataset Source: We used a diabetes dataset,from kaggle website.
*   Features:Pregnancies, Glucose, Blood Pressure, Skin Thickness, Insulin, BMI,Age
*   Target Variable: Binary outcome indicating whether the patient has diabetes (1) or not (0).
*   Size: The dataset comprises 768 instances with 8 features.
*   Data Type: All features are numerical.
*   Missing Data: Some records have missing values in features like glucose, insulin, etc., which were handled using imputation.
*   Purpose: The dataset helps train and evaluate machine learning models to predict diabetes risk effectively.

# **Data loading**
"""

from google.colab import drive
drive.mount("/content/drive")

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import KFold
import warnings
warnings.simplefilter(action='ignore')
sns.set()
plt.style.use("ggplot")
# %matplotlib inline

# read the dataset from directory
df=pd.read_csv("/content/drive/My Drive/ML Project/diabetes.csv")
df.head()

"""# **EDA Part**"""

# provides a summary of our DataSet
df.info()

df.columns

# independent feature->'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
#        'BMI', 'DiabetesPedigreeFunction', 'Age'
# dependent feature-> outcome

# generates summary statistics for numerical columns in the DataSet
df.describe()

# (row, columns)
df.shape

# shows the percentage distribution of the values in the 'Outcome' column of the DataSet
print((df['Outcome'].value_counts() * 100 / len(df)).rename("Outcome"))

for column in df.columns:
    print(f"MAX {column}: {df[column].max()}")
    print(f"MIN {column}: {df[column].min()}")

# display the distribution of 8 features using histograms
fig,ax = plt.subplots(4,2, figsize=(20,20))
sns.distplot(df.Pregnancies, bins=20, ax=ax[0,0], color="blue")
sns.distplot(df.Glucose, bins=20, ax=ax[0,1], color="blue")
sns.distplot(df.BloodPressure, bins=20, ax=ax[1,0], color="blue")
sns.distplot(df.SkinThickness, bins=20, ax=ax[1,1], color="blue")
sns.distplot(df.Insulin, bins=20, ax=ax[2,0], color="blue")
sns.distplot(df.BMI, bins=20, ax=ax[2,1], color="blue")
sns.distplot(df.DiabetesPedigreeFunction, bins=20, ax=ax[3,0], color="blue")
sns.distplot(df.Age, bins=20, ax=ax[3,1], color="blue")

# average number of pregnancies for each unique value in the "Outcome" column.
df.groupby("Outcome").agg({'Pregnancies':'mean'})

# maximum number of pregnancies for each unique value in the "Outcome" column.
df.groupby("Outcome").agg({'Pregnancies':'max'})

# average glucose levels for each outcome (diabetes or no diabetes)
df.groupby("Outcome").agg({'Glucose':'mean'})

# returns the highest glucose level for each outcome (diabetes vs. no diabetes)
df.groupby("Outcome").agg({'Glucose':'max'})

# 0>healthy
# 1>diabetes
# create two visualizations(pie chart and count plot) for the "Outcome" column in the dataset
f,ax = plt.subplots(1,2, figsize=(18,8))
df['Outcome'].value_counts().plot.pie(explode=[0,0.1],autopct = "%1.1f%%", ax=ax[0], shadow=True)
ax[0].set_title('target')
ax[0].set_ylabel('')
#sns.countplot('Outcome', data=df, ax=ax[1])
sns.countplot(x='Outcome', data=df, ax=ax[1])
ax[1].set_title('Outcome')
plt.show()

# calculate the correlation matrix for the numerical columns in the dataset. This is useful for understanding relationships between variables
df.corr()

# generates a heatmap to visualize the correlation matrix of numerical columns in the dataset
f,ax = plt.subplots(figsize=[20,15])
sns.heatmap(df.corr(), annot=True, fmt = '.2f', ax=ax, cmap='magma')
ax.set_title("Correlation Matrix", fontsize=20)
plt.show()

"""# **Data Preprocessing Part**"""

df.columns

# Replaces all 0 values in the selected columns with NaN (Not a Number)
df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age']] = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age']].replace(0, np.NaN)

# returns the total count of missing values (NaN) in each column of the dataset
df.isnull().sum()

# displays the first 5 rows of the dataset
df.head()

# create a bar chart visualization of missing data in the dataset
import missingno as msno
msno.bar(df, color="blue")

# compare the median of a variable for different target outcomes.
def median_target(var):
    temp = df[df[var].notnull()]
    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()
    return temp

# fills missing values with the respective group medians based on the "Outcome" class
columns = df.columns
columns = columns.drop("Outcome")
for i in columns:
    median_target(i)
    df.loc[(df['Outcome'] == 0 ) & (df[i].isnull()), i] = median_target(i)[i][0]
    df.loc[(df['Outcome'] == 1 ) & (df[i].isnull()), i] = median_target(i)[i][1]

df.head()

# returns the number of missing (NaN) values in each column of the dataset
df.isnull().sum()

# creates a pair plot using Seaborn to visualize relationships between pairs of variables
p = sns.pairplot(df, hue="Outcome")

# checks for outliers in each feature
for feature in df:
    Q1 = df[feature].quantile(0.25)
    Q3 = df[feature].quantile(0.75)
    IQR = Q3-Q1
    lower = Q1-1.5*IQR
    upper = Q3+1.5*IQR
    if df[(df[feature]>upper)].any(axis=None):
        print(feature, "yes")
    else:
        print(feature, "no")

# identifying outliers in the "Insulin" feature.
plt.figure(figsize=(8,7))
sns.boxplot(x= df["Insulin"], color="red")

# calculates the Interquartile Range (IQR) for the "Insulin" column
Q1 = df.Insulin.quantile(0.25)
Q3 = df.Insulin.quantile(0.75)
IQR = Q3-Q1
lower = Q1-1.5*IQR
upper = Q3+1.5*IQR
df.loc[df['Insulin']>upper, "Insulin"] = upper

# Boxplot without outliers
plt.figure(figsize=(8,7))
sns.boxplot(x= df["Insulin"], color="red")

# LOF - local outlier factor (machine learning algorithm to detect outliers)
# It returns an array where -1 indicates an outlier and 1 indicates an inlier.
from sklearn.neighbors import LocalOutlierFactor
lof = LocalOutlierFactor(n_neighbors=10)
lof.fit_predict(df)

df.head()

# creates a boxplot for the "Pregnancies" column
plt.figure(figsize=(8,7))
sns.boxplot(x= df["Pregnancies"], color="red")

# identify the most extreme outliers in all variables in the dataset.
df_scores = lof.negative_outlier_factor_
np.sort(df_scores)[0:20]

# sets a threshold value for identifying outliers.The threshold helps to define what LOF score range to classify as an outlier.
thresold = np.sort(df_scores)[7]

thresold

# identifying the points that are considered outliers based on the threshold value set earlier
outlier = df_scores>thresold

# filters the DataFrame df to keep only the rows that are identified as outliers based on the outlier condition
df = df[outlier]

df.head()

df.shape

plt.figure(figsize=(8,7))
sns.boxplot(x= df["Pregnancies"], color="red")

"""# Feature engineering"""

# Creates a categorical series called NewBMI with defined categories for BMI levels such as "Underweight," "Normal," and various levels of obesity.
NewBMI = pd.Series(pd.Categorical(
    ["Underweight", "Normal", "Overweight", "Obesity 1", "Obesity 2", "Obesity 3"],
    categories=["Normal", "Obesity 1", "Obesity 2", "Obesity 3", "Overweight", "Underweight"],
    ordered=True
))

# confirms the ordering and content of the categorical series.
print(NewBMI)

# Groups individuals into predefined BMI categories for easier analysis and comparisons.
df['NewBMI'] = NewBMI
df.loc[df["BMI"]<18.5, "NewBMI"] = NewBMI[0]
df.loc[(df["BMI"]>18.5) & df["BMI"]<=24.9, "NewBMI"] = NewBMI[1]
df.loc[(df["BMI"]>24.9) & df["BMI"]<=29.9, "NewBMI"] = NewBMI[2]
df.loc[(df["BMI"]>29.9) & df["BMI"]<=34.9, "NewBMI"] = NewBMI[3]
df.loc[(df["BMI"]>34.9) & df["BMI"]<=39.9, "NewBMI"] = NewBMI[4]
df.loc[df["BMI"]>39.9, "NewBMI"] = NewBMI[5]

df.head()

# This function is designed to label insulin levels as "Normal" or "Abnormal," enabling easier analysis or classification based on insulin measurements.
def set_insuline(row):
    if row["Insulin"]>=16 and row["Insulin"]<=166:
        return "Normal"
    else:
        return "Abnormal"

# This creates a new feature, NewInsulinScore, which categorizes insulin levels for each record, making it easier to analyze.
df = df.assign(NewInsulinScore=df.apply(set_insuline, axis=1))

df.head()

# Converts numerical glucose levels into meaningful categories for better analysis in models.
NewGlucose = pd.Series(["Low", "Normal", "Overweight", "Secret", "High"], dtype = "category")
df["NewGlucose"] = NewGlucose
df.loc[df["Glucose"] <= 70, "NewGlucose"] = NewGlucose[0]
df.loc[(df["Glucose"] > 70) & (df["Glucose"] <= 99), "NewGlucose"] = NewGlucose[1]
df.loc[(df["Glucose"] > 99) & (df["Glucose"] <= 126), "NewGlucose"] = NewGlucose[2]
df.loc[df["Glucose"] > 126 ,"NewGlucose"] = NewGlucose[3]

df.head()

"""# **One Hot Encoding**"""

# converts the categorical columns into one-hot encoded columns
df = pd.get_dummies(df, columns = ["NewBMI", "NewInsulinScore", "NewGlucose"], drop_first=True)

df.head()

df.columns

#  columns represent one-hot encoded categorical features related to BMI, Insulin Score, and Glucose levels are selected to create a new DataFrame categorical_df
categorical_df = df[['NewBMI_Obesity 1',
       'NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight',
       'NewBMI_Underweight', 'NewInsulinScore_Normal', 'NewGlucose_Low',
       'NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret']]

categorical_df.head()

# The code selects the target variable (Outcome) as y and the other features as X, removing certain one-hot encoded columns to avoid redundancy
y=df['Outcome']
X=df.drop(['Outcome','NewBMI_Obesity 1',
       'NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight',
       'NewBMI_Underweight', 'NewInsulinScore_Normal', 'NewGlucose_Low',
       'NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret'], axis=1)

# The code saves the column names and index of the features into cols and index for later use
cols = X.columns
index = X.index

X.head()

"""# **Feature Scalling**"""

# The code scales the features in X using RobustScaler, which handles outliers better.
# After scaling, the data is transformed and then converted back into a DataFrame, retaining the original columns and index.
from sklearn.preprocessing import RobustScaler
transformer = RobustScaler().fit(X)
X=transformer.transform(X)
X=pd.DataFrame(X, columns = cols, index = index)

X.head()

# combines the numerical feature set (X) with the categorical feature set (categorical_df) by adding the categorical columns as new columns to X.
# This prepares the data for model training by including both numerical and categorical features.
X = pd.concat([X, categorical_df], axis=1)

X.head()

#splits the dataset into training and testing sets. It uses 80% of the data for training and 20% for testing. The random_state=0 ensures the split is reproducible.
X_train, X_test, y_train , y_test = train_test_split(X,y, test_size=0.2, random_state=0)

# This ensures that both datasets are standardized similarly
scaler =StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""# **Model Implementation**

# **Logistic Regression**
"""

# The code trains a logistic regression model on the training data. It learns patterns in the data to make predictions about the outcome variable.
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

# uses the trained logistic regression modelto make predictions on the test data
y_pred = log_reg.predict(X_test)

# calculates the accuracy of the logistic regression model on the training data
accuracy_score(y_train, log_reg.predict(X_train))

# calculates the accuracy of the logistic regression model on the test data
accuracy_score(y_test, log_reg.predict(X_test))

# calculates the accuracy of the Logistic Regression model on the test data
log_reg_acc = accuracy_score(y_test, log_reg.predict(X_test))

# calculates and prints the confusion matrix, which helps evaluate the performance of the logistic regression model
conf_matrix = np.array(confusion_matrix(y_test, y_pred), dtype=np.int64)
print(f"array({conf_matrix.tolist()}, dtype=int64)")

# provides key metrics such as precision, recall, F1-score, and support for each class. It helps evaluate model performance, especially in imbalanced datasets
print(classification_report(y_test, y_pred))

"""# **Decision Tree**"""

# trains a decision tree classifier, makes predictions on both training and test data,
# and evaluates its performance using accuracy scores, confusion matrix, and classification report
DT = DecisionTreeClassifier()
DT.fit(X_train, y_train)
y_pred = DT.predict(X_test)
print(accuracy_score(y_train, DT.predict(X_train)))

print(accuracy_score(y_test, DT.predict(X_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# performs hyperparameter tuning for a DecisionTreeClassifier using GridSearchCV
grid_param = {
    'criterion':['gini','entropy'],
    'max_depth' :  [3,5,7,10],
    'splitter' : ['best','radom'],
    'min_samples_leaf':[1,2,3,5,7],
    'min_samples_split':[1,2,3,5,7],
    'max_features':['auto','sqrt','log2']
}
grid_search_dt = GridSearchCV(DT, grid_param, cv=50, n_jobs=-1, verbose = 1)
grid_search_dt.fit(X_train, y_train)

# returns the best hyperparameters for the DecisionTreeClassifier after running GridSearchCV on the parameter grid
grid_search_dt.best_params_

# returns the best cross-validation score achieved by the DecisionTreeClassifier based on the hyperparameters chosen during the grid search
# Cross-validation is a technique used to assess how well a model generalizes to new, unseen data
grid_search_dt.best_score_

DT = grid_search_dt.best_estimator_                   # Retrieves the best decision tree model from the grid search
y_pred = DT.predict(X_test)                           # Predicts the outcomes for the test set using the trained model
print(accuracy_score(y_train, DT.predict(X_train)))   # Prints the accuracy score for the training set predictions
dt_acc = accuracy_score(y_test, DT.predict(X_test))   # Calculates and stores the accuracy score for the test set predictions
print(accuracy_score(y_test, DT.predict(X_test)))     # Prints the accuracy score for the test set predictions
print(confusion_matrix(y_test, y_pred))               # Prints the confusion matrix to evaluate the model’s performance
print(classification_report(y_test, y_pred))          # Prints a detailed classification report (precision, recall, F1-score) for the test set

"""# **Random Forest**"""

#  initializes a Random Forest classifier with specified parameters, and then fits it using the training data
rand_clf = RandomForestClassifier(criterion = 'entropy', max_depth = 15, max_features = 0.75, min_samples_leaf = 2, min_samples_split = 3, n_estimators = 130)
rand_clf.fit(X_train, y_train)

# uses the trained Random Forest classifier to make predictions on the test data and stores the predicted results in y_pred
y_pred = rand_clf.predict(X_test)

# The code uses the Random Forest model to make predictions on the test set.
# It calculates and prints the accuracy on both the training and test data, and provides the confusion matrix and classification report to evaluate the model's performance.
y_pred = rand_clf.predict(X_test)
print(accuracy_score(y_train, rand_clf.predict(X_train)))
rand_acc = accuracy_score(y_test, rand_clf.predict(X_test))
print(accuracy_score(y_test, rand_clf.predict(X_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""# **Model Evaluation and Discussion**"""

# compare and evaluate the models based on their performance
models = pd.DataFrame({
    'Model': ['Logistic Regression', 'Decision Tree Classifier', 'Random Forest Classifier'],
    'Score': [100*round(log_reg_acc, 4), 100*round(dt_acc, 4), 100*round(rand_acc, 4)]
})
models.sort_values(by='Score', ascending=False)

# This code plots the ROC (Receiver Operating Characteristic) curves for three models.
# It calculates the False Positive Rate (FPR) and True Positive Rate (TPR) for each model and displays their AUC (Area Under the Curve) scores.
# The resulting plot compares the performance of the models, helping to evaluate their classification accuracy visually.
from sklearn import metrics
plt.figure(figsize=(8,5))
models = [
    {'label': 'LR', 'model': log_reg},
    {'label': 'DT', 'model': DT},
    {'label': 'RF', 'model': rand_clf}
]
for m in models:
    model = m['model']
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    fpr1, tpr1, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:, 1])
    auc = metrics.roc_auc_score(y_test, model.predict(X_test))
    plt.plot(fpr1, tpr1, label='%s - ROC (area = %0.2f)' % (m['label'], auc))

plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1 - Specificity (False Positive Rate)', fontsize=12)
plt.ylabel('Sensitivity (True Positive Rate)', fontsize=12)
plt.title('ROC - Diabetes Prediction', fontsize=12)
plt.legend(loc="lower right", fontsize=12)
plt.savefig("roc_diabetes.jpeg", format='jpeg', dpi=400, bbox_inches='tight')
plt.show()

from sklearn import metrics
import numpy as np
import matplotlib.pyplot as plt

# Models to evaluate
models = [
    {'label': 'LR', 'model': log_reg},
    {'label': 'DT', 'model': DT},
    {'label': 'RF', 'model': rand_clf}
]

# Initialize accuracy and ROC lists
means_accuracy = [100*round(log_reg_acc, 4), 100*round(dt_acc, 4), 100*round(rand_acc, 4)]
means_roc = []

# Calculate accuracy and ROC for each model
for m in models:
    model = m['model']
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    fpr1, tpr1, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:, 1])
    auc = metrics.roc_auc_score(y_test, model.predict(X_test))
    auc = 100*round(auc, 4)
    means_roc.append(auc)

print("Accuracy scores: ", means_accuracy)
print("ROC scores: ", means_roc)

# Data for the plot
n_groups = 3  # Only 3 models
means_accuracy = tuple(means_accuracy)
means_roc = tuple(means_roc)

# Create the plot
fig, ax = plt.subplots(figsize=(8,5))
index = np.arange(n_groups)
bar_width = 0.35
opacity = 0.8

# Plot the bars
rects1 = plt.bar(index, means_accuracy, bar_width, alpha=opacity, color='mediumpurple', label='Accuracy (%)')
rects2 = plt.bar(index + bar_width, means_roc, bar_width, alpha=opacity, color='rebeccapurple', label='ROC (%)')

# Customize plot
plt.xlim([-1, 3])
plt.ylim([60, 95])
plt.title('Performance Evaluation - Diabetes Prediction', fontsize=12)
plt.xticks(index + bar_width / 2, ('LR', 'DT', 'RF'), rotation=0, ha='center', fontsize=12)
plt.legend(loc="upper right", fontsize=10)

# Save and show plot
plt.savefig("PE_diabetes.jpeg", format='jpeg', dpi=400, bbox_inches='tight')
plt.show()

"""# **Conclusion**

The Diabetes Prediction project effectively demonstrates the potential of machine learning in healthcare diagnostics. By employing Logistic Regression, Decision Tree Classifier, and Random Forest Classifier, the project accurately predicts diabetes risk using patient data. Through data preprocessing, feature selection, and model evaluation, it highlights the importance of technology in early detection and prevention. The findings provide valuable insights for medical professionals, helping to improve patient outcomes and resource allocation in healthcare systems.

So, in conclusion we can see that random forest classifier has higher accuracy and ROC value than decision tree and logistic regression. So, Random forest is better for evaluatioin process based on our dataset.

# **References**

*   https://en.wikipedia.org/wiki/Random_forest
*   https://en.wikipedia.org/wiki/Decision_tree_learning
*   https://en.wikipedia.org/wiki/Logistic_regression
"""